{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1\n",
    "\n",
    "- Simple Linear Regression은 하나의 정보로부터 하나의 Regression을 얻음\n",
    "- Multivariate Linear Regression으로 해결!\n",
    " \n",
    "# Hypothesis Fuction \n",
    "H(x) = w1x1 + w2x2 + w3x3 + b \n",
    "-> 입력변수가 여러 개면 weight도 그에 맞게!\n",
    "\n",
    "- 입력변수가 여러 개임을 감안하여 matmul() 함수로 계산\n",
    "- 속도가 빠름!\n",
    "\n",
    "# Cost function: MSE\n",
    "- 기존 simple linear Regression과 동일한 공식\n",
    "\n",
    "# 경사하강 공부\n",
    "\n",
    "# nn.Module\n",
    "1. nn.module을 상속해서 모델 생성\n",
    "2. nn.Linear(3,1)\n",
    "    - 입력 차원: 3\n",
    "    - 출력 차원: 1\n",
    "3. Hypothesis 계산은 forward() 에서!\n",
    "4. Gradient 계산은 알아서 PyTorch가 해준다! (backward())\n",
    "\n",
    "## F.mse_loss\n",
    "1. torch.nn.funtional에서 제공하는 loss function 사용\n",
    "2. 쉽게 다른 loss와 교체 가능!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20, hypothesis: tensor([0., 0., 0., 0., 0.]), Cost: 29661.800781\n",
      "Epoch    1/20, hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]), Cost: 9298.520508\n",
      "Epoch    2/20, hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]), Cost: 2915.712402\n",
      "Epoch    3/20, hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]), Cost: 915.040649\n",
      "Epoch    4/20, hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]), Cost: 287.936157\n",
      "Epoch    5/20, hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]), Cost: 91.371010\n",
      "Epoch    6/20, hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]), Cost: 29.758249\n",
      "Epoch    7/20, hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]), Cost: 10.445281\n",
      "Epoch    8/20, hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]), Cost: 4.391237\n",
      "Epoch    9/20, hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]), Cost: 2.493121\n",
      "Epoch   10/20, hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]), Cost: 1.897688\n",
      "Epoch   11/20, hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]), Cost: 1.710555\n",
      "Epoch   12/20, hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]), Cost: 1.651412\n",
      "Epoch   13/20, hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]), Cost: 1.632369\n",
      "Epoch   14/20, hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]), Cost: 1.625924\n",
      "Epoch   15/20, hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]), Cost: 1.623420\n",
      "Epoch   16/20, hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]), Cost: 1.622141\n",
      "Epoch   17/20, hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]), Cost: 1.621262\n",
      "Epoch   18/20, hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]), Cost: 1.620501\n",
      "Epoch   19/20, hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]), Cost: 1.619764\n",
      "Epoch   20/20, hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]), Cost: 1.619046\n"
     ]
    }
   ],
   "source": [
    "#데이터 입력\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93], \n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100], \n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "#모델 초기화\n",
    "w = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD([w, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #H(x) 계산\n",
    "    hypothesis = x_train.matmul(w) + b\n",
    "\n",
    "    #cost 계산\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{}, hypothesis: {}, Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 점점 작아지는 Cost\n",
    "- 점점 y에 가까워지는 H(x)\n",
    "- Learning rate에 따라 발산할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn. Module\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "    def forward (self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F.mse_loss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20, hypothesis: tensor([-4.9883, -4.2813, -5.2218, -4.0710, -4.2004]), Cost: 31236.568359\n",
      "Epoch    1/20, hypothesis: tensor([64.0312, 78.6757, 76.5168, 84.9402, 59.0751]), Cost: 9792.588867\n",
      "Epoch    2/20, hypothesis: tensor([102.6726, 125.1204, 122.2792, 134.7743,  94.5008]), Cost: 3071.040527\n",
      "Epoch    3/20, hypothesis: tensor([124.3063, 151.1232, 147.8999, 162.6746, 114.3345]), Cost: 964.192200\n",
      "Epoch    4/20, hypothesis: tensor([136.4181, 165.6813, 162.2439, 178.2948, 125.4389]), Cost: 303.806396\n",
      "Epoch    5/20, hypothesis: tensor([143.1989, 173.8319, 170.2746, 187.0400, 131.6560]), Cost: 96.810120\n",
      "Epoch    6/20, hypothesis: tensor([146.9951, 178.3952, 174.7705, 191.9360, 135.1368]), Cost: 31.927719\n",
      "Epoch    7/20, hypothesis: tensor([149.1203, 180.9502, 177.2877, 194.6771, 137.0858]), Cost: 11.589964\n",
      "Epoch    8/20, hypothesis: tensor([150.3100, 182.3807, 178.6969, 196.2116, 138.1771]), Cost: 5.214869\n",
      "Epoch    9/20, hypothesis: tensor([150.9758, 183.1817, 179.4858, 197.0707, 138.7882]), Cost: 3.216269\n",
      "Epoch   10/20, hypothesis: tensor([151.3485, 183.6303, 179.9274, 197.5516, 139.1305]), Cost: 2.589423\n",
      "Epoch   11/20, hypothesis: tensor([151.5570, 183.8815, 180.1747, 197.8208, 139.3223]), Cost: 2.392614\n",
      "Epoch   12/20, hypothesis: tensor([151.6736, 184.0223, 180.3130, 197.9715, 139.4299]), Cost: 2.330547\n",
      "Epoch   13/20, hypothesis: tensor([151.7387, 184.1012, 180.3904, 198.0558, 139.4902]), Cost: 2.310746\n",
      "Epoch   14/20, hypothesis: tensor([151.7751, 184.1455, 180.4337, 198.1029, 139.5242]), Cost: 2.304176\n",
      "Epoch   15/20, hypothesis: tensor([151.7952, 184.1704, 180.4579, 198.1293, 139.5434]), Cost: 2.301755\n",
      "Epoch   16/20, hypothesis: tensor([151.8064, 184.1844, 180.4715, 198.1440, 139.5542]), Cost: 2.300671\n",
      "Epoch   17/20, hypothesis: tensor([151.8125, 184.1924, 180.4790, 198.1521, 139.5605]), Cost: 2.299960\n",
      "Epoch   18/20, hypothesis: tensor([151.8158, 184.1969, 180.4831, 198.1566, 139.5641]), Cost: 2.299374\n",
      "Epoch   19/20, hypothesis: tensor([151.8174, 184.1996, 180.4855, 198.1591, 139.5663]), Cost: 2.298841\n",
      "Epoch   20/20, hypothesis: tensor([151.8183, 184.2012, 180.4867, 198.1604, 139.5677]), Cost: 2.298322\n"
     ]
    }
   ],
   "source": [
    "# 데이터 입력\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93], \n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100], \n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "#모델 초기화\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    #cost 계산\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{}, hypothesis: {}, Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2\n",
    "\n",
    "- 큰 데이터를 불러와 학습하는 방법은 !?\n",
    "-> Minibatch Gradient Descent\n",
    "\n",
    "# Minibatch Gradient Descent\n",
    "- 전체 데이터를 균일하게 나눠서 학습하자!\n",
    "\n",
    "- 업데이트를 좀 더 빠르게 할 수 있음\n",
    "- 전체 데이터를 쓰지 않아서 잘못된 방향으로 업데이트를 할 수도 있음"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92adb05438905df6de4687daafcf1cce5d284141f7aba8feef918d2743df949f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
