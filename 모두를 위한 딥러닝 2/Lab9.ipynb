{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "\n",
    "## Problem of Sigmoid\n",
    "Input -> Network(Sigmoid) -> Output\n",
    "- Sigmoid는 양 끝 값이 작음 -> Gradient 너무 작아짐 = Vanishing Gradient\n",
    "\n",
    "## ReLU\n",
    ": f(x) = max(0, x)\n",
    "- 양수: 자기 자신\n",
    "- 음수: 0 \n",
    "- leaky_relu는 음의 영역에서 0값만 내는 부분을 변형한 것\n",
    "\n",
    "# Optimizer \n",
    "- 구현해보기!\n",
    "\n",
    "# 코드\n",
    "- optim에서 조정 가능 (optim.Adam)\n",
    "- torch.nn.ReLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root = 'MNIST_data/', train=True, transform= transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root = 'MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "data_loader = DataLoader(dataset=mnist_train, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(784, 10, bias=True)\n",
    "\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(linear.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 5.820862770\n",
      "Epoch:  0002 cost = 1.757082105\n",
      "Epoch:  0003 cost = 1.129140615\n",
      "Epoch:  0004 cost = 0.880213857\n",
      "Epoch:  0005 cost = 0.742414594\n",
      "Epoch:  0006 cost = 0.653183043\n",
      "Epoch:  0007 cost = 0.591368139\n",
      "Epoch:  0008 cost = 0.545832634\n",
      "Epoch:  0009 cost = 0.510411084\n",
      "Epoch:  0010 cost = 0.481701791\n",
      "Epoch:  0011 cost = 0.458988845\n",
      "Epoch:  0012 cost = 0.439683139\n",
      "Epoch:  0013 cost = 0.423185378\n",
      "Epoch:  0014 cost = 0.409159899\n",
      "Epoch:  0015 cost = 0.397321969\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/total_batch\n",
    "    print('Epoch: ', \"%.4d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "#마지막에는 crossEntropyLoss 때문에 Softmax\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 171.441741943\n",
      "Epoch:  0002 cost = 40.457157135\n",
      "Epoch:  0003 cost = 25.461828232\n",
      "Epoch:  0004 cost = 17.746196747\n",
      "Epoch:  0005 cost = 12.896922112\n",
      "Epoch:  0006 cost = 9.579449654\n",
      "Epoch:  0007 cost = 7.059277534\n",
      "Epoch:  0008 cost = 5.324962616\n",
      "Epoch:  0009 cost = 4.186229706\n",
      "Epoch:  0010 cost = 3.027694941\n",
      "Epoch:  0011 cost = 2.411275148\n",
      "Epoch:  0012 cost = 1.749804020\n",
      "Epoch:  0013 cost = 1.457693696\n",
      "Epoch:  0014 cost = 1.110923767\n",
      "Epoch:  0015 cost = 0.828423381\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost/total_batch\n",
    "    print('Epoch: ', \"%.4d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitht Initialization\n",
    "## Need to set the initial weight values wisely\n",
    "- 0으로 초기화 하는 것 X\n",
    "- RBM을 이용한 후 weight 조정을 하면 성능이 올라감\n",
    "\n",
    "## Restricted Boltzmann Machine\n",
    "- Restricted: no connection within a layer\n",
    "- KL divergence: compare actual to recreation\n",
    "\n",
    "## Pretraning\n",
    "\n",
    "## Xavier/ He initialization\n",
    "- 새로운 초기화 방법\n",
    "\n",
    "<Xavier>\n",
    "\n",
    "1. Xavier Normal initialization\n",
    "2. Xavier Uniform initialization\n",
    "\n",
    "<br>\n",
    "\n",
    "<He>\n",
    "\n",
    "1. He Normal initialization\n",
    "2. He Uniform initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "#마지막에는 crossEntropyLoss 때문에 Softmax\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 0.241877362\n",
      "Epoch:  0002 cost = 0.092874795\n",
      "Epoch:  0003 cost = 0.061129767\n",
      "Epoch:  0004 cost = 0.043883067\n",
      "Epoch:  0005 cost = 0.034479223\n",
      "Epoch:  0006 cost = 0.026350787\n",
      "Epoch:  0007 cost = 0.022326684\n",
      "Epoch:  0008 cost = 0.020252341\n",
      "Epoch:  0009 cost = 0.013746651\n",
      "Epoch:  0010 cost = 0.015260044\n",
      "Epoch:  0011 cost = 0.011925268\n",
      "Epoch:  0012 cost = 0.011711510\n",
      "Epoch:  0013 cost = 0.011446440\n",
      "Epoch:  0014 cost = 0.010251939\n",
      "Epoch:  0015 cost = 0.010605388\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost/total_batch\n",
    "    print('Epoch: ', \"%.4d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92adb05438905df6de4687daafcf1cce5d284141f7aba8feef918d2743df949f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
