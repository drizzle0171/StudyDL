{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "\n",
    "## Problem of Sigmoid\n",
    "Input -> Network(Sigmoid) -> Output\n",
    "- Sigmoid는 양 끝 값이 작음 -> Gradient 너무 작아짐 = Vanishing Gradient\n",
    "\n",
    "## ReLU\n",
    ": f(x) = max(0, x)\n",
    "- 양수: 자기 자신\n",
    "- 음수: 0 \n",
    "- leaky_relu는 음의 영역에서 0값만 내는 부분을 변형한 것\n",
    "\n",
    "# Optimizer \n",
    "- 구현해보기!\n",
    "\n",
    "# 코드\n",
    "- optim에서 조정 가능 (optim.Adam)\n",
    "- torch.nn.ReLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "drop_prob = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root = 'MNIST_data/', train=True, transform= transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root = 'MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "data_loader = DataLoader(dataset=mnist_train, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(784, 10, bias=True)\n",
    "\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(linear.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 4.669600964\n",
      "Epoch:  0002 cost = 1.522242665\n",
      "Epoch:  0003 cost = 1.052284241\n",
      "Epoch:  0004 cost = 0.851526320\n",
      "Epoch:  0005 cost = 0.733133495\n",
      "Epoch:  0006 cost = 0.653148413\n",
      "Epoch:  0007 cost = 0.595389247\n",
      "Epoch:  0008 cost = 0.551395476\n",
      "Epoch:  0009 cost = 0.516230345\n",
      "Epoch:  0010 cost = 0.489193708\n",
      "Epoch:  0011 cost = 0.466469258\n",
      "Epoch:  0012 cost = 0.446874678\n",
      "Epoch:  0013 cost = 0.430808157\n",
      "Epoch:  0014 cost = 0.416153491\n",
      "Epoch:  0015 cost = 0.403240830\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/total_batch\n",
    "    print('Epoch: ', \"%.4d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "#마지막에는 crossEntropyLoss 때문에 Softmax\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 152.080307007\n",
      "Epoch:  0002 cost = 35.916236877\n",
      "Epoch:  0003 cost = 22.403242111\n",
      "Epoch:  0004 cost = 15.801609993\n",
      "Epoch:  0005 cost = 11.485497475\n",
      "Epoch:  0006 cost = 8.410363197\n",
      "Epoch:  0007 cost = 6.160271168\n",
      "Epoch:  0008 cost = 4.706752777\n",
      "Epoch:  0009 cost = 3.477179050\n",
      "Epoch:  0010 cost = 2.723419666\n",
      "Epoch:  0011 cost = 2.097049475\n",
      "Epoch:  0012 cost = 1.502471685\n",
      "Epoch:  0013 cost = 1.234463930\n",
      "Epoch:  0014 cost = 0.915809393\n",
      "Epoch:  0015 cost = 0.714687765\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost/total_batch\n",
    "    print('Epoch: ', \"%.4d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitht Initialization\n",
    "## Need to set the initial weight values wisely\n",
    "- 0으로 초기화 하는 것 X\n",
    "- RBM을 이용한 후 weight 조정을 하면 성능이 올라감\n",
    "\n",
    "## Restricted Boltzmann Machine\n",
    "- Restricted: no connection within a layer\n",
    "- KL divergence: compare actual to recreation\n",
    "\n",
    "## Pretraning\n",
    "\n",
    "## Xavier/ He initialization\n",
    "- 새로운 초기화 방법\n",
    "\n",
    "<Xavier>\n",
    "\n",
    "1. Xavier Normal initialization\n",
    "2. Xavier Uniform initialization\n",
    "\n",
    "<br>\n",
    "\n",
    "<He>\n",
    "\n",
    "1. He Normal initialization\n",
    "2. He Uniform initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "#마지막에는 crossEntropyLoss 때문에 Softmax\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 0.240409985\n",
      "Epoch:  0002 cost = 0.091164231\n",
      "Epoch:  0003 cost = 0.059896652\n",
      "Epoch:  0004 cost = 0.043436833\n",
      "Epoch:  0005 cost = 0.031374697\n",
      "Epoch:  0006 cost = 0.026184672\n",
      "Epoch:  0007 cost = 0.018392703\n",
      "Epoch:  0008 cost = 0.020304648\n",
      "Epoch:  0009 cost = 0.017290844\n",
      "Epoch:  0010 cost = 0.013786529\n",
      "Epoch:  0011 cost = 0.011858572\n",
      "Epoch:  0012 cost = 0.010539472\n",
      "Epoch:  0013 cost = 0.010296844\n",
      "Epoch:  0014 cost = 0.011762344\n",
      "Epoch:  0015 cost = 0.010060399\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost/total_batch\n",
    "    print('Epoch: ', \"%.4d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "## Overfitting\n",
    "<-> underfitting\n",
    ": overfitting, underfitting 모두 문제!\n",
    ": 훈련 데이터에만 집중 되어 있다보니, 테스트 데이터셋에서 좋지 못한 성능을 냄\n",
    "\n",
    "## 해결방법: Dropout!\n",
    ": 각 학습의 노드를 무작위로 껐다 켰다 하는 방법\n",
    "1. 학습 데이터 x가 주어짐\n",
    "2. 각 레이어에 dropout이 적용됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p= drop_prob) #probability: 각 스텝에서 사용하지 않을 노드의 확률을 정해주는 파라미터\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(linear1, relu, dropout, \n",
    "                            linear2, relu, dropout, \n",
    "                            linear3)\n",
    "#마지막에는 crossEntropyLoss 때문에 Softmax\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "## Gradient Vanishing/Exploding\n",
    "- Vanishing: Gradient가 점점 작아져 소멸해지는 상태\n",
    "- Exploding: Gradient가 점점 커지는 상태\n",
    "\n",
    "## Batch Normalization\n",
    "### Internal Covariate Shift\n",
    ": 훈련 세트와 테스트 세트의 분포가 차이가 있다!\n",
    "- 레이어마다 데이터의 분포가 달라짐\n",
    "- 한 레이어마다 covariate shift가 발생한다!\n",
    "- 각 배치마다 Normalization을 해주겠다 ! = Batch Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "bn1 = torch.nn.BatchNorm1d(32)\n",
    "bn2 = torch.nn.BatchNorm1d(32)\n",
    "\n",
    "bn_model = torch.nn.Sequential(linear1,  bn1, relu, \n",
    "                               linear2, bn2, relu, \n",
    "                               linear3)\n",
    "#마지막에는 crossEntropyLoss 때문에 Softmax\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92adb05438905df6de4687daafcf1cce5d284141f7aba8feef918d2743df949f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
