{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro 인공신경망\n",
    ": 뉴런의 동작방식을 본따 만든 모델\n",
    "\n",
    "- 입력신호\n",
    "- 특정 값을 넘으면 다음 뉴런으로 넘어감\n",
    "\n",
    "# Perceptron\n",
    "- 입력값이 들어옴\n",
    "- 입력값에 가중치를 곱함\n",
    "- 가중치의 합 + 편향\n",
    "- activation function으로 output\n",
    "-> 한 레이어로만 된 perceptron은 xor 문제를 해결할 수 없다!\n",
    "\n",
    "# XOR\n",
    "- 서로 다른 입력이 들어오면 1\n",
    "- 서로 같은 입력이 들어오면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7109015583992004\n",
      "100 0.6931483149528503\n",
      "200 0.6931471824645996\n",
      "300 0.6931471824645996\n",
      "400 0.6931471824645996\n",
      "500 0.6931471824645996\n",
      "600 0.6931471824645996\n",
      "700 0.6931471824645996\n",
      "800 0.6931471824645996\n",
      "900 0.6931471824645996\n",
      "1000 0.6931471824645996\n",
      "1100 0.6931471824645996\n",
      "1200 0.6931471824645996\n",
      "1300 0.6931471824645996\n",
      "1400 0.6931471824645996\n",
      "1500 0.6931471824645996\n",
      "1600 0.6931471824645996\n",
      "1700 0.6931471824645996\n",
      "1800 0.6931471824645996\n",
      "1900 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "2100 0.6931471824645996\n",
      "2200 0.6931471824645996\n",
      "2300 0.6931471824645996\n",
      "2400 0.6931471824645996\n",
      "2500 0.6931471824645996\n",
      "2600 0.6931471824645996\n",
      "2700 0.6931471824645996\n",
      "2800 0.6931471824645996\n",
      "2900 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "3100 0.6931471824645996\n",
      "3200 0.6931471824645996\n",
      "3300 0.6931471824645996\n",
      "3400 0.6931471824645996\n",
      "3500 0.6931471824645996\n",
      "3600 0.6931471824645996\n",
      "3700 0.6931471824645996\n",
      "3800 0.6931471824645996\n",
      "3900 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "4100 0.6931471824645996\n",
      "4200 0.6931471824645996\n",
      "4300 0.6931471824645996\n",
      "4400 0.6931471824645996\n",
      "4500 0.6931471824645996\n",
      "4600 0.6931471824645996\n",
      "4700 0.6931471824645996\n",
      "4800 0.6931471824645996\n",
      "4900 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "5100 0.6931471824645996\n",
      "5200 0.6931471824645996\n",
      "5300 0.6931471824645996\n",
      "5400 0.6931471824645996\n",
      "5500 0.6931471824645996\n",
      "5600 0.6931471824645996\n",
      "5700 0.6931471824645996\n",
      "5800 0.6931471824645996\n",
      "5900 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "6100 0.6931471824645996\n",
      "6200 0.6931471824645996\n",
      "6300 0.6931471824645996\n",
      "6400 0.6931471824645996\n",
      "6500 0.6931471824645996\n",
      "6600 0.6931471824645996\n",
      "6700 0.6931471824645996\n",
      "6800 0.6931471824645996\n",
      "6900 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "7100 0.6931471824645996\n",
      "7200 0.6931471824645996\n",
      "7300 0.6931471824645996\n",
      "7400 0.6931471824645996\n",
      "7500 0.6931471824645996\n",
      "7600 0.6931471824645996\n",
      "7700 0.6931471824645996\n",
      "7800 0.6931471824645996\n",
      "7900 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "8100 0.6931471824645996\n",
      "8200 0.6931471824645996\n",
      "8300 0.6931471824645996\n",
      "8400 0.6931471824645996\n",
      "8500 0.6931471824645996\n",
      "8600 0.6931471824645996\n",
      "8700 0.6931471824645996\n",
      "8800 0.6931471824645996\n",
      "8900 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "9100 0.6931471824645996\n",
      "9200 0.6931471824645996\n",
      "9300 0.6931471824645996\n",
      "9400 0.6931471824645996\n",
      "9500 0.6931471824645996\n",
      "9600 0.6931471824645996\n",
      "9700 0.6931471824645996\n",
      "9800 0.6931471824645996\n",
      "9900 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "#xor 문제를 해결하는 perceptron 코드\n",
    "\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "#perceptron은 하나의 레이어를 가짐\n",
    "linear = torch.nn.Linear(2,1,bias=True)\n",
    "#activation function\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    ": 여러 층의 레이어로 이루어진 모델\n",
    "\n",
    "# Backpropagation\n",
    ": weight에 대한 cost(loss)의 미분값을 최소화할 수 있도록 weight를 업데이트를 하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8490627408027649\n",
      "100 0.6935698390007019\n",
      "200 0.693418562412262\n",
      "300 0.6933491230010986\n",
      "400 0.6933169364929199\n",
      "500 0.69330233335495\n",
      "600 0.6932964324951172\n",
      "700 0.6932944655418396\n",
      "800 0.693294107913971\n",
      "900 0.6932942867279053\n",
      "1000 0.6932945251464844\n",
      "1100 0.6932944059371948\n",
      "1200 0.693294107913971\n",
      "1300 0.6932933330535889\n",
      "1400 0.6932923197746277\n",
      "1500 0.6932910084724426\n",
      "1600 0.6932895183563232\n",
      "1700 0.69328773021698\n",
      "1800 0.6932858824729919\n",
      "1900 0.6932839155197144\n",
      "2000 0.6932818293571472\n",
      "2100 0.6932796239852905\n",
      "2200 0.6932774782180786\n",
      "2300 0.6932750940322876\n",
      "2400 0.6932727098464966\n",
      "2500 0.6932703256607056\n",
      "2600 0.6932679414749146\n",
      "2700 0.6932655572891235\n",
      "2800 0.6932631134986877\n",
      "2900 0.693260669708252\n",
      "3000 0.6932581663131714\n",
      "3100 0.6932556629180908\n",
      "3200 0.6932531595230103\n",
      "3300 0.6932506561279297\n",
      "3400 0.6932481527328491\n",
      "3500 0.6932456493377686\n",
      "3600 0.693243145942688\n",
      "3700 0.6932405233383179\n",
      "3800 0.6932380199432373\n",
      "3900 0.6932355165481567\n",
      "4000 0.6932330131530762\n",
      "4100 0.693230390548706\n",
      "4200 0.6932278871536255\n",
      "4300 0.6932253837585449\n",
      "4400 0.6932227611541748\n",
      "4500 0.6932202577590942\n",
      "4600 0.6932176351547241\n",
      "4700 0.6932151317596436\n",
      "4800 0.693212628364563\n",
      "4900 0.6932100057601929\n",
      "5000 0.6932075023651123\n",
      "5100 0.6932049989700317\n",
      "5200 0.6932024955749512\n",
      "5300 0.693199872970581\n",
      "5400 0.6931973695755005\n",
      "5500 0.6931948065757751\n",
      "5600 0.6931923031806946\n",
      "5700 0.6931896805763245\n",
      "5800 0.6931871175765991\n",
      "5900 0.6931846141815186\n",
      "6000 0.693182110786438\n",
      "6100 0.6931796073913574\n",
      "6200 0.6931771039962769\n",
      "6300 0.6931744813919067\n",
      "6400 0.6931719779968262\n",
      "6500 0.693169355392456\n",
      "6600 0.6931668519973755\n",
      "6700 0.6931643486022949\n",
      "6800 0.6931617259979248\n",
      "6900 0.6931592226028442\n",
      "7000 0.6931565999984741\n",
      "7100 0.6931542158126831\n",
      "7200 0.693151593208313\n",
      "7300 0.6931490898132324\n",
      "7400 0.6931464672088623\n",
      "7500 0.693143904209137\n",
      "7600 0.6931414008140564\n",
      "7700 0.6931388974189758\n",
      "7800 0.6931363344192505\n",
      "7900 0.6931337118148804\n",
      "8000 0.6931312084197998\n",
      "8100 0.6931285858154297\n",
      "8200 0.6931260824203491\n",
      "8300 0.6931235790252686\n",
      "8400 0.6931208968162537\n",
      "8500 0.6931184530258179\n",
      "8600 0.6931158304214478\n",
      "8700 0.6931132078170776\n",
      "8800 0.6931106448173523\n",
      "8900 0.693108081817627\n",
      "9000 0.6931054592132568\n",
      "9100 0.6931029558181763\n",
      "9200 0.6931003332138062\n",
      "9300 0.693097710609436\n",
      "9400 0.6930950880050659\n",
      "9500 0.6930924654006958\n",
      "9600 0.6930898427963257\n",
      "9700 0.6930872201919556\n",
      "9800 0.6930845975875854\n",
      "9900 0.6930819749832153\n"
     ]
    }
   ],
   "source": [
    "#Backpropagation\n",
    "\n",
    "learning_rate = 0.1\n",
    "X = torch.FloatTensor([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "#직접 가중치, 편향 제공\n",
    "w1 = torch.Tensor(2,2)\n",
    "b1 = torch.Tensor(2)\n",
    "w2 = torch.Tensor(2,1)\n",
    "b2 = torch.Tensor(1)\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "for step in range(10000):\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid_function(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid_function(l2)\n",
    "\n",
    "    cost = -torch.mean(Y*torch.log(Y_pred)+(1-Y)*torch.log(1-Y_pred))\n",
    "\n",
    "    #back prop\n",
    "    d_Y_pred = (Y_pred-Y) / (Y_pred * (1.0 - Y_pred) + 1e-7) #미분\n",
    "\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
    "\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1) \n",
    "\n",
    "    #weight update\n",
    "    w1 = w1 - learning_rate * d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "    w2 = w2 - learning_rate * d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.695796012878418\n",
      "100 0.6932618021965027\n",
      "200 0.6931381225585938\n",
      "300 0.6930206418037415\n",
      "400 0.6927193999290466\n",
      "500 0.691414475440979\n",
      "600 0.6806575059890747\n",
      "700 0.5543442964553833\n",
      "800 0.18834306299686432\n",
      "900 0.07622473686933517\n",
      "1000 0.04495593160390854\n",
      "1100 0.03136700019240379\n",
      "1200 0.023928917944431305\n",
      "1300 0.01927829347550869\n",
      "1400 0.016110315918922424\n",
      "1500 0.01381980162113905\n",
      "1600 0.012089603580534458\n",
      "1700 0.010738191194832325\n",
      "1800 0.009654389694333076\n",
      "1900 0.008766476064920425\n",
      "2000 0.008026083000004292\n",
      "2100 0.0073994738049805164\n",
      "2200 0.006862532813102007\n",
      "2300 0.006397380493581295\n",
      "2400 0.005990613251924515\n",
      "2500 0.005631947889924049\n",
      "2600 0.005313347093760967\n",
      "2700 0.005028524436056614\n",
      "2800 0.004772416781634092\n",
      "2900 0.004540855530649424\n",
      "3000 0.004330559633672237\n",
      "3100 0.004138701595366001\n",
      "3200 0.003962996415793896\n",
      "3300 0.0038014152087271214\n",
      "3400 0.0036524254828691483\n",
      "3500 0.0035145855508744717\n",
      "3600 0.0033866793382912874\n",
      "3700 0.0032676714472472668\n",
      "3800 0.003156691789627075\n",
      "3900 0.0030529892537742853\n",
      "4000 0.0029557549860328436\n",
      "4100 0.0028645386919379234\n",
      "4200 0.0027787405997514725\n",
      "4300 0.002697881544008851\n",
      "4400 0.002621586201712489\n",
      "4500 0.002549420576542616\n",
      "4600 0.002481099683791399\n",
      "4700 0.002416369039565325\n",
      "4800 0.002354854252189398\n",
      "4900 0.002296405378729105\n",
      "5000 0.0022407679352909327\n",
      "5100 0.0021877323742955923\n",
      "5200 0.002137119183316827\n",
      "5300 0.002088808221742511\n",
      "5400 0.0020426351111382246\n",
      "5500 0.0019984054379165173\n",
      "5600 0.0019560442306101322\n",
      "5700 0.001915461616590619\n",
      "5800 0.0018765227869153023\n",
      "5900 0.0018391236662864685\n",
      "6000 0.0018031439976766706\n",
      "6100 0.0017685540951788425\n",
      "6200 0.001735264086164534\n",
      "6300 0.0017031993484124541\n",
      "6400 0.001672270125709474\n",
      "6500 0.0016424614004790783\n",
      "6600 0.0016136984340846539\n",
      "6700 0.00158592127263546\n",
      "6800 0.001559055526740849\n",
      "6900 0.0015330859459936619\n",
      "7000 0.0015079976292327046\n",
      "7100 0.0014836860354989767\n",
      "7200 0.0014601211296394467\n",
      "7300 0.001437347731553018\n",
      "7400 0.0014152016956359148\n",
      "7500 0.001393742742948234\n",
      "7600 0.0013729108031839132\n",
      "7700 0.0013527360279113054\n",
      "7800 0.0013331137597560883\n",
      "7900 0.0013140738010406494\n",
      "8000 0.0012955265119671822\n",
      "8100 0.0012775317300111055\n",
      "8200 0.0012600148329511285\n",
      "8300 0.0012429459020495415\n",
      "8400 0.0012263546232134104\n",
      "8500 0.0012101961765438318\n",
      "8600 0.00119444087613374\n",
      "8700 0.0011791186407208443\n",
      "8800 0.00116415461525321\n",
      "8900 0.0011495787184685469\n",
      "9000 0.0011353909503668547\n",
      "9100 0.0011214867699891329\n",
      "9200 0.0011079558171331882\n",
      "9300 0.0010947234695777297\n",
      "9400 0.0010818194132298231\n",
      "9500 0.0010692437645047903\n",
      "9600 0.0010568919824436307\n",
      "9700 0.0010448386892676353\n",
      "9800 0.0010330838849768043\n",
      "9900 0.0010215828660875559\n"
     ]
    }
   ],
   "source": [
    "#xor 문제를 해결하는 perceptron 코드\n",
    "\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "#multi-perceptron은 여러 층의 레이어를 가짐\n",
    "linear1 = torch.nn.Linear(2,2,bias=True)\n",
    "linear2 = torch.nn.Linear(2,1,bias=True) #MLP\n",
    "\n",
    "#activation function\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, cost.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92adb05438905df6de4687daafcf1cce5d284141f7aba8feef918d2743df949f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
